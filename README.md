# hip_llama.cpp

Inference llama2 model on the AMD GPUs system

## Getting Started

- Install the dependencies.
- Clone the repository.
- Run the project.

```
cd hip_llama.cpp
make
```

## Usage

- Instructions on how to use the project.
- Examples of how to inference llama2.

```
runcc model.bin -m test -f <input_filename> -o <output_filename>
```

## Documentation

- Not available yet.

## Contributing

- The project will open for contributer soon. If you have some issues or feature requests, please open an issue tickets.

## License

[GPL-3.0](https://www.gnu.org/licenses/gpl-3.0.html)

## Contributers

| Full Name        | Email                     | ID     |
| ---------------- | ------------------------- | ------ |
| Pham Manh Tien   | tien.pham@moreh.com.vn    | getp16 |
| Nguyen Huy Hoang | hoang.nguyen@moreh.com.vn | getp11 |
| Nguyen Xuan Anh  | anh.nguyen@moreh.com.vn   | getp15 |

## Acknowledgments

Reference:

- [llama2.c](https://github.com/karpathy/llama2.c)
- [llama2.c for Dummies](https://github.com/RahulSChand/llama2.c-for-dummies?tab=readme-ov-file)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
